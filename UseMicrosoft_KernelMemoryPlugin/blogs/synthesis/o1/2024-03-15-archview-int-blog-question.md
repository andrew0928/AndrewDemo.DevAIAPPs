## FAQ

Q1: 什麼是 RAG (Retrieval-Augmented Generation)，它在文章中扮演什麼角色？  
A1: RAG 是將知識檢索與大型語言模型結合的流程。文章強調其三階段：1) Ingestion：先把大篇幅文章轉成向量並建立索引；2) Retrieval：將使用者問題也向量化，找出最相近的文章片段；3) Synthesis：將檢索結果交由 LLM 生成易讀且精準的回覆。這樣能讓需要搜尋龐雜文本的應用，更能直覺且精準地回答使用者問題，無需人工在海量內容中尋找。作者以自家部落格 327 篇、累計 400 萬字的內容作範例，示範如何把傳統「關鍵字搜尋」進化成更聰明的「語意檢索＋生成」模式。

Q2: 作者如何利用 GPTs 與 Kernel Memory 整合實做出智慧型檢索？  
A2: 作者在 GPTs（基於 Chat GPT 平台）的自訂 instruction 與 Function Calling 機制下，提供一個可呼叫外部檢索 API 的介面。這個檢索服務背後採用 Kernel Memory，先把作者所有文章 chunks 做向量化並存進向量資料庫。當使用者在 GPTs 對話中提出 query，GPTs 自動呼叫 /search API，把最相關的幾段文字傳回，然後再用 GPT 產出最終回答。路徑上實際運用了 Azure OpenAI 的 text-embedding-3-large 模型，並套用了 GPT-4 進行最終的文字生成。整合後可以協助讀者快速搜尋與彙整複雜、多年的文章內容。

Q3: 在文章中，作者提到 Tag 與 Filters，有什麼目的？  
A3: Tag 與 Filters 是用來做內容的屬性標示與權限控管。Kernel Memory 允許在文件匯入階段添加自訂標籤，例如「user-tags」、「categories」等，並在檢索時透過 filters 做二元邏輯（AND/OR）過濾。這在大量文件或是具有不同存取需求的環境中特別重要，能用來做分眾、內容分類或存取限制（ABAC）。作者強調這機制可實現在向量資料庫仍不提供 record-level 權限時的彈性解。

Q4: 文章提到資料庫在 AI 時代的演進，為何要從 RDB、NoSQL 走到 VectorDB？  
A4: 傳統 RDB 以表格化和正規化為核心，NoSQL 提供文件或物件導向的存取方式，降低 join 需求並強調開發貼近業務。而隨著 AI 與語言模型的成熟，人們需要更強的「語意搜尋」功能。VectorDB 讓文件（或任何資料）可在多維度向量空間比對相似度，進行語意檢索。RAG 即應此而生，把高階語意搜尋能力結合到應用程式中，極大提升資料挖掘與利用效率。這並非取代 RDB 或 NoSQL，而是補足它們在語意分析上的不足。

Q5: 如果我想開發自己的搜尋/問答服務，為何不直接用 /ask 而要再搭配 GPTs？  
A5: Kernel Memory 提供 /ask API，可同時做檢索與回答。但作者演示了 GPTs 作前端介面的優勢：第一，有對話式上下文，可在多輪對話中保留先前問題；第二，負擔 GPT-4 推論的運算費用端由用戶的 Chat GPT Plus 角色承擔，減輕開發者雲端費用壓力；第三，利用 GPTs 可直接把 API 功能繫於 Chat 界面上，用戶不需切換系統。若只需要單次問答或不要求對話式體驗，也可以自行調用 /ask 完成訊息產生。

Q6: 使用 GPTs 最大的瓶頸或考量是什麼？  
A6: 首先是成本與使用者門檻。GPTs 的功能綁定在 Chat GPT Plus 帳號，並非所有人都有訂閱；另外大語言模型推論（尤其是 GPT-4）Token 成本不低，若長期、頻繁地使用可能造成費用壓力。其次，若需要更複雜或專屬的安全控管，必須在檢索服務與應用程式邏輯層面額外擴充，或是考量自行建置私有化解決方案。最後，對於需要極度客製化或離線部署的情況，GPTs 平台仍有侷限，但可先作為概念驗證的利器。

Q7: 文章裡多次提到 Embedding 與向量化，核心意義是什麼？  
A7: Embedding 透過深度學習模型，把句子、段落乃至圖片等轉換成多維向量（如 3072 維），使內容在高維空間中能用向量相似度衡量「語意相似度」。這些向量資料（或稱「向量索引」）存入 VectorDB，當用戶提出問題時，只要把問題也轉成向量，接著在空間裡找最相近區域，即能在海量內容中篩選出最相關文段。其關鍵價值在於可跳脫關鍵字侷限，更真實地反映語句含義或概念上的相似。

Q8: 若公司資料龐大且部分機密，該怎麼導入文中 RAG 機制？  
A8: 公司可先規劃清楚哪些文檔適合導入向量化與共享，可先為機密與公開文檔標注 Tags。對內部人員（經認證）才能查詢某些 Tag 資料，用 API 過濾控管。若要完全私有化，也可在公司內網建置 Azure OpenAI On Your Data 或同類私有模型，必須準備 GPU 硬體及部署熟練度。檢索與 LLM 推論部分若要避免外洩，可選擇私有向量資料庫，並考慮規劃資料脫敏、權限策略後再進行 Ingestion。

Q9: 如何避免 RAG 中出現捏造內容或亂回答的狀況？  
A9: 首先，Retrieval 阶段應有足夠嚴謹的向量過濾門檻（如設定 minRelevanceLimit），避免取回與 query 毫不相關的段落。其次，Synthesis 時可在 Prompt 裡面規定「若事實不足則回答 'INFO NOT FOUND'」。另外，也可明確要求回答必附引文來源（Citation），使用者便能比對回答是否真有在原文。若對模型行為有額外需求，可進一步做 Prompt Engineering 或考慮 Fine-Tune 讓回答更嚴謹。

Q10: 作者對未來架構師應培養哪些能力，有何建議？  
A10: 文章最後點出，如今系統開發不只要懂資料結構與最傳統的程式組合，也需要通曉 LLM、向量搜尋、語意分析等新要素，才能真正把 AI 整合到應用程式。作者特別強調若只一味使用 Chat GPT 產程式，恐失去深度理解優勢；真正掌握 AI 原理、知道怎麼結合商業需求、懂得控成本與安全風險，是架構師在未來十年最寶貴的競爭力。  